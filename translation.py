# -*- coding: utf-8 -*-
"""Translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCX6j-flVd-t1cU16ndTbQNL9Mx67gto
"""

!pip install PyPDF2

import  PyPDF2  as pdf

file=open('/content/EN_presto_1um_1212_20130121_mail.pdf','rb')
file

pdf_reader= pdf.PdfReader(file)

help(pdf_reader)

print(len(pdf_reader.pages))

page1 = pdf_reader.pages[0]
page2=pdf_reader.pages[1]

page1.extract_text ()

page2.extract_text ()

"""#Pdf writer to write the extract text into the new pdf"""

pdf_writer=pdf.PdfWriter ()

pdf_writer.add_page (page1)
pdf_writer.add_page (page2)

output=open('pages.pdf','wb')
pdf_writer.write(output)
output.close()

import PyPDF2
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
filename = '/content/EN_presto_1um_1212_20130121_mail.pdf'
open_filename = open(filename, 'rb')

ind_manifesto = PyPDF2.PdfReader (open_filename)

total_pages = len(ind_manifesto.pages)
total_pages

!pip install textract

import textract

count = 0
text  = ''

# Lets loop through, to read each page from the pdf file
while(count < total_pages):
    # Get the specified number of pages in the document
    mani_page  = ind_manifesto.pages[count]
    # Process the next page
    count += 1
    # Extract the text from the page
    text += mani_page.extract_text ()
    if text != '':

      text = text

    else:

      textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )

!pip install autocorrect

!pip install pymupdf

import fitz
my_path = '/content/EN_presto_1um_1212_20130121_mail.pdf'
doc = fitz.open(my_path)
for page in doc:
    text = page.get_text()
    print(text)

output = page.get_text("blocks")
for page in doc:
    output = page.get_text("blocks")
    previous_block_id = 0 # Set a variable to mark the block id
    for  block in output:
        if block[6] == 0: # We only take the text
            if previous_block_id != block[5]:
                # Compare the block number
                print("\n")
            print(block[4])

!pip install Unidecode

from unidecode import unidecode
output = []
for page in doc:
    output += page.get_text("blocks")
previous_block_id = 0 # Set a variable to mark the block id
for block in output:
  if block[6] == 0:
     # We only take the text
    if previous_block_id != block[5]: # Compare the block number
       print("\n")
       plain_text = unidecode(block[4])
       print(plain_text)

block_dict = {}
page_num = 1
for page in doc: # Iterate all pages in the document
    file_dict = page.get_text('dict') # Get the page dictionary
    block = file_dict['blocks'] # Get the block information
    block_dict[page_num] = block # Store in block dictionary
    page_num += 1 # Increase the page value by 1

import re
import pandas as pd
import numpy as np
spans = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'text', 'tag'])
rows = []
for page_num, blocks in block_dict.items():
    for block in blocks:
        if block['type'] == 0:
            for line in block['lines']:
                for span in line['spans']:
                    xmin, ymin, xmax, ymax = list(span['bbox'])
                    font_size = span['size']
                    text = unidecode(span['text'])
                    span_font = span['font']
                    is_upper = False
                    is_bold = False
                    if "bold" in span_font.lower():
                        is_bold = True
                    if re.sub("[\(\[].*?[\)\]]", "", text).isupper():
                        is_upper = True
                    if text.replace(" ","") !=  "":
                        rows.append((xmin, ymin, xmax, ymax, text,                              is_upper, is_bold, span_font, font_size))
                        span_df = pd.DataFrame(rows, columns=['xmin','ymin','xmax','ymax', 'text', 'is_upper','is_bold','span_font', 'font_size'])

span_df

span_scores = []

span_num_occur = {}

special = '[(_:/,#%\=@)]'




for index, span_row in span_df.iterrows():



    score = round(span_row.font_size)

    text = span_row.text



    if not re.search(special, text):



        if span_row.is_bold:

            score +=1




        if span_row.is_upper:

            score +=1



    span_scores.append(score)




values, counts = np.unique(span_scores, return_counts=True)

values, counts = np.unique(span_scores, return_counts=True)

style_dict = {}

for value, count in zip(values, counts):

    style_dict[value] = count

sorted(style_dict.items(), key=lambda x: x[1])

p_size = max(style_dict, key=style_dict.get)




idx = 0

tag = {}




for size in sorted(values, reverse = True):

    idx += 1

    if size == p_size:

        idx = 0

        tag[size] = 'p'

    if size > p_size:

        tag[size] = 'h{0}'.format(idx)

    if size < p_size:

        tag[size] = 's{0}'.format(idx)

span_tags = [tag[score] for score in span_scores]

span_df['tag'] = span_tags

span_df

headings_list = []

text_list = []

tmp = []

heading = ''

for index, span_row in span_df.iterrows():

    text = span_row.text

    tag = span_row.tag

    if 'h' in tag:

        headings_list.append(text)

        text_list.append('\n'.join(tmp))

        tmp = []

        heading = text

    else:

        tmp.append(text)

text_list.append('\n'.join(tmp))


text_list = text_list[1:]

text_df = pd.DataFrame(zip(headings_list, text_list),columns=['heading', 'content'] )

print(text_list)

text_df.head(30)

headings_list

text_df.head

headings_list[4]

text_list[4]

# Combine the heading and text lists
combined_text = ""
for i in range(len(headings_list)):
    combined_text += headings_list[i] + "\n" + text_list[i] + "\n\n"

print(combined_text)

!pip install datasets evaluate transformers[sentencepiece]

! pip install sentencepiece

!pip freeze | grep transformers

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-one-to-many-mmt")

tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-one-to-many-mmt", src_lang="en_XX")

# Split the combined text into 1024-length chunks
max_length = 1024
chunks = [combined_text[i:i+max_length] for i in range(0, len(combined_text), max_length)]

for chunk in chunks:
    print(chunk)
    print("---")

model_inputs = tokenizer(chunk, return_tensors="pt")

generated_tokens = model.generate(
    **model_inputs,
    forced_bos_token_id=tokenizer.lang_code_to_id["de_DE"],
    min_new_tokens=1000 # Set the maximum number of new tokens
)

translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

translation

generated_tokens = model.generate(
    **model_inputs,
    forced_bos_token_id=tokenizer.lang_code_to_id["ta_IN"],
    min_new_tokens=1000 # Set the maximum number of new tokens
)

translation_TN = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

translation_TN

pip install sacrebleu

from datasets import load_metric
bleu = load_metric('bleu')

sacrebleu = load_metric('sacrebleu')

predictions = ['mit elektronischen Systemen ohne schriftliche Einwilligung gedruckt. Konstruktion und Fertigung / Typen- und Druckpräsentation: TecDoc GmbH; Geislingen, info@tecdocgmbh.de Original Bedienungsanleitung. Gedruckt in Deutschland. Wir behalten uns das Recht vor, technische Änderungen vor. Ihr nächstgelegener WMF Service: 21.01.2013']
references = ['über elektronische Systeme ohne schriftliche Zustimmung verbreitet werden.Gestaltung und Produktion / Satz und Druckvorlage:TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Original-Benutzerhandbuch. Gedruckt in Deutschland.Technische Änderungen behalten wir uns vor.Ihr nächstgelegener WMF Service: 21.01.2013']
sacrebleu.compute(predictions=[predictions], references=[references])

predictions = [['mit', 'elektronischen', 'Systemen' ,'ohne', 'schriftliche', 'Einwilligung gedruckt']]
references = [[['über', 'elektronische', 'Systeme', 'ohne', 'schriftliche', 'Zustimmung', 'verbreitet', 'werden']]]
bleu.compute(predictions=predictions, references=references)

! pip install transformers

from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")

[{'translation_text': 'Par défaut, développer les fils de discussion'}]

translator(
    "distributed using electronic systems without written consent. Design and production / typesetting and print presentation: TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Original User Manual. Printed in Germany. We reserve the right to make technical modifications. Your nearest WMF Service:  21.01.2013."
)

reference = ["distribués par le biais de systèmes électroniques sans autorisation écrite. Conception et production / composition et présentation à l'impression : TecDoc GmbH ; Geislingen, info@tecdocgmbh.de Manuel d'utilisation original. Imprimé en Allemagne. Nous nous réservons le droit d'apporter des modifications techniques. Service WMF le plus proche :  21.01.2013"]

[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]

predictions = ["distribué en utilisant des systèmes électroniques sans autorisation écrite. Conception et production / composition de composition et présentation d'impression & #160;: TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Manuel original de l'utilisateur. Imprimé en Allemagne. Nous nous réservons le droit de faire des modifications techniques. Votre service WMF le plus proche & #160;: 21.01.13."]
references = ["distribués par le biais de systèmes électroniques sans autorisation écrite. Conception et production / composition et présentation à l'impression : TecDoc GmbH ; Geislingen, info@tecdocgmbh.de Manuel d'utilisation original. Imprimé en Allemagne. Nous nous réservons le droit d'apporter des modifications techniques. Service WMF le plus proche : 21.01.2013"]
sacrebleu.compute(predictions=[predictions], references=[references])

predictions = ["distribué", "en", "utilisant", "des", "systèmes", "électroniques", "sans", "autorisation écrite"]
references = ["distribués", "par", "le", "biais", "de", "systèmes", "électroniques", "sans autorisation écrite"]
bleu.compute(predictions=[predictions], references=[[references]])

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")
translator = pipeline("translation", model=model, tokenizer=tokenizer)
translator("distributed using electronic systems without written consent. Design and production / typesetting and print presentation: TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Original User Manual. Printed in Germany. We reserve the right to make technical modifications. Your nearest WMF Service:  21.01.2013.")

predictions = ["distribué en utilisant des systèmes électroniques sans autorisation écrite. Conception et production / composition de composition et présentation d'impression & #160;: TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Manuel original de l'utilisateur. Imprimé en Allemagne. Nous nous réservons le droit de faire des modifications techniques. Votre service WMF le plus proche & #160;: 21.01.13."]
references = ["über elektronische Systeme ohne schriftliche Zustimmung verbreitet werden. Gestaltung und Produktion / Satz und Druckvorlage:TecDoc GmbH; Geislingen, info@tecdocgmbh.de. Original-Benutzerhandbuch. Gedruckt in Deutschland.Technische Änderungen behalten wir uns vor.Ihr nächstgelegener WMF Service:21.01.2013"]
sacrebleu.compute(predictions=[predictions], references=[references])

predictions = ["Vertrieb", "mit", "elektronischen", "Systemen", "ohne", "schriftliche", "Zustimmung"]
references = ["über", "elektronische", "Systeme", "ohne", "schriftliche", "Zustimmung", "verbreitet", "werden"]
bleu.compute(predictions=[predictions], references=[[references]])

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("unicamp-dl/translation-pt-en-t5")

model = AutoModelForSeq2SeqLM.from_pretrained("unicamp-dl/translation-pt-en-t5")

pten_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)

pten_pipeline("translate Portuguese to English: distribuídos por meio de sistemas eletrônicos sem consentimento por escrito.")

predictions = ["distributed by means of electronic systems without written consent"]
references = ["distributed using electronic systems without written consent."]
sacrebleu.compute(predictions=[predictions], references=[references])

predictions = ["distributed", "by", "means", "of", "electronic", "systems", "without", "written", "consent"]
references = ["distributed", "using", "electronic", "systems", "without", "written", "consent."]
bleu.compute(predictions=[predictions], references=[[references]])

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/bert2bert_L-24_wmt_de_en", pad_token="<pad>", eos_token="</s>", bos_token="<s>")
model = AutoModelForSeq2SeqLM.from_pretrained("google/bert2bert_L-24_wmt_de_en")

sentence = "über elektronische Systeme ohne schriftliche Zustimmung verbreitet werden."

input_ids = tokenizer(sentence, return_tensors="pt", add_special_tokens=False).input_ids
output_ids = model.generate(input_ids)[0]
print(tokenizer.decode(output_ids, skip_special_tokens=True))

predictions = ["via electronic systems without written consent."]
references = ["über elektronische Systeme ohne schriftliche Zustimmung verbreitet werden."]
sacrebleu.compute(predictions=[predictions], references=[references])

predictions = ["via", "electronic", "systems", "without", "written", "consent."]
references = ["über", "elektronische", "Systeme", "ohne", "schriftliche", "Zustimmung", "verbreitet", "werden."]
bleu.compute(predictions=[predictions], references=[[references]])

